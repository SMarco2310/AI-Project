{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SMarco2310/AI-Project/blob/branch2_tracy/Intro_to_AI_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Processing**\n",
        "\n",
        "\n",
        " We try to rescale all the picture to 160x160 to make the use of the face net algorithm easy\n",
        "\n",
        " # **Steps:**\n",
        "\n",
        "## **Steps 1: Create and new folder to save the changes we made**\n",
        "\n",
        "- Create new directory for the transformed data\n",
        "- read the data\n",
        "\n",
        "- create a function that resize the pictures\n",
        "\n",
        "## **Step2: Ensure uniform color format and resize all the picture to 160x160**\n",
        "- resize all the pictures to the 160*160 format\n",
        "- create a function that changes the color of the picture to RGB color format\n",
        "\n",
        "## Step3: Change the image format to PNG\n",
        "- Create a function to convert all the picture from whatever format to PNG\n"
      ],
      "metadata": {
        "id": "MfzoNd19Ac45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Libraries to be used for this task\n",
        "! pip install opencv-python\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5eIYxF5CumI",
        "outputId": "1e4c5125-4867-4d71-84a0-ec9d74d4d8ce"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is to have before and after view of the picture\n",
        "\n",
        "# Before\n",
        "\n",
        "display_picture = cv2.imread(\"/content/sample_data/0001_01.jpg\")\n",
        "img= cv2.cvtColor(display_picture, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "iA7A0x-RHebH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resizing function"
      ],
      "metadata": {
        "id": "H4bjXpBTLwHo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "5si1R3F-9FpZ"
      },
      "outputs": [],
      "source": [
        "def resize_image(picture_name:str):\n",
        "   img = cv2.imread(picture_name)\n",
        "   # This resizes the picture to the 160x160 format\n",
        "   img = cv2.resize(img, (160,160))\n",
        "   cv2.imwrite(\"resized_\"+picture_name, img)\n",
        "   # This ensures the RGB color format\n",
        "   img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "   return img"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After\n",
        "img1 = resize_image(\"/content/sample_data/0001_01.jpg\")\n",
        "plt.imshow(img1)"
      ],
      "metadata": {
        "id": "YrxlLycKGP7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a code I found to create new directory:"
      ],
      "metadata": {
        "id": "TfPwlBLBAGfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code should be edited so that when looping throught the directory,the changes will be applied to the files and they will be moved to new directory"
      ],
      "metadata": {
        "id": "TM651D-paIOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import colab\n",
        "colab.drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YztiA_uGwsB2",
        "outputId": "b562601c-cc58-4d98-9b28-54d068829878"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_image(picture_path: str, output_path: str):\n",
        "    img = cv2.imread(picture_path)\n",
        "    img = cv2.resize(img, (160, 160))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    cv2.imwrite(output_path, img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def loop_through_folders(input_dir, output_dir):\n",
        "    \"\"\"\n",
        "    Loops through the folders in the given directory, processes images,\n",
        "    and outputs them to a new directory maintaining the structure.\n",
        "    \"\"\"\n",
        "    for root, dirs, files in os.walk(input_dir):\n",
        "        # Compute relative path from input_dir to current root\n",
        "        rel_path = os.path.relpath(root, input_dir)\n",
        "        # Create equivalent directory inside output_dir\n",
        "        target_dir = os.path.join(output_dir, rel_path)\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "        for file in files:\n",
        "            if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "                file = str(file)[0] + \".png\"\n",
        "                input_path = os.path.join(root, file)\n",
        "                output_path = os.path.join(target_dir, file)\n",
        "                resize_image(input_path, output_path)\n",
        "\n",
        "# Example usage:\n",
        "input_directory = \"/content/drive/MyDrive/val\"\n",
        "output_directory = \"/content/drive/MyDrive/archive (2)/TransformedData\"\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "loop_through_folders(input_directory, output_directory)"
      ],
      "metadata": {
        "id": "j7qz09VOZzHj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ke0CQwqScCfc",
        "outputId": "6e2f6584-9f7d-44a6-96aa-4cfe963e6fcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRE TRAINED FACENET ALGORITHM** (INCEPTION RESNET)"
      ],
      "metadata": {
        "id": "A3TG6LAjeYpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a basic Convolutional Layer for reuse in blocks\n",
        "class FacenetLayer(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0):\n",
        "        super(FacenetLayer, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_planes, eps=0.001, momentum=0.1, affine=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "#This code tells the model how to dela with the data\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "#first block that deals with pattern detection\n",
        "class Block35(nn.Module):\n",
        "    def __init__(self, scale=1.0):\n",
        "        super(Block35, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.branch0 = FacenetLayer(256, 32, kernel_size=1)\n",
        "        #Creates a sequence of layers\n",
        "        self.branch1 = nn.Sequential(\n",
        "            FacenetLayer(256, 32, kernel_size=1),\n",
        "            FacenetLayer(32, 32, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            FacenetLayer(256, 64, kernel_size=1),\n",
        "            FacenetLayer(64, 64, kernel_size=3, padding=1),\n",
        "            FacenetLayer(64, 64, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.conv2d = nn.Conv2d(128, 256, kernel_size=1, stride=1)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y0 = self.branch0(x)\n",
        "        y1 = self.branch1(x)\n",
        "        y2 = self.branch2(x)\n",
        "        out = torch.cat((y0, y1, y2), 1)\n",
        "        out = self.conv2d(out)\n",
        "        out = out * self.scale + x\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Block17(nn.Module):\n",
        "    def __init__(self, scale=1.0):\n",
        "        super(Block17, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.branch0 = FacenetLayer(896, 128, kernel_size=1)\n",
        "        self.branch1 = nn.Sequential(\n",
        "            FacenetLayer(256, 32, kernel_size=1),\n",
        "            FacenetLayer(128, 128, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            FacenetLayer(128, 128, kernel_size=(7, 1), padding=(3, 0))\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            FacenetLayer(896, 128, kernel_size=1),\n",
        "            FacenetLayer(128, 160, kernel_size=3, padding=1),\n",
        "            FacenetLayer(160, 192, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.conv2d = nn.Conv2d(256, 896, kernel_size=1, stride=1)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y0 = self.branch0(x)\n",
        "        y1 = self.branch1(x)\n",
        "        y2 = self.branch2(x)\n",
        "        #concatenates the three branches into one dimension\n",
        "        out = torch.cat((y0, y1, y2), 1)\n",
        "        out = self.conv2d(out)\n",
        "        out = out * self.scale + x\n",
        "        #Activation function to create non linearity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Block8(nn.Module):\n",
        "    def __init__(self, scale=1.0, noReLU=False):\n",
        "        super(Block8, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.noReLU = noReLU\n",
        "        self.branch0 = FacenetLayer(1792, 192, kernel_size=1)\n",
        "        self.branch1 = nn.Sequential(\n",
        "            FacenetLayer(1792, 192, kernel_size=1),\n",
        "            FacenetLayer(192, 192, kernel_size=(1, 3), padding=(1, 0)),\n",
        "            FacenetLayer(192, 192, kernel_size=(3, 1), padding=(0, 1))\n",
        "        )\n",
        "        self.conv2d = nn.Conv2d(384, 1792, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y0 = self.branch0(x)\n",
        "        y1 = self.branch1(x)\n",
        "        out = torch.cat((y0, y1), 1)\n",
        "        out = self.conv2d(out)\n",
        "        out = out * self.scale + x\n",
        "        if not self.noReLU:\n",
        "            out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "#Special blocks\n",
        "class Mixed_6a(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mixed_6a, self).__init__()\n",
        "        self.branch0 = FacenetLayer(256, 384, kernel_size=3, stride=2)\n",
        "        self.branch1 = nn.Sequential(\n",
        "            FacenetLayer(256, 192, kernel_size=1),\n",
        "            FacenetLayer(192, 192, kernel_size=3, padding=1),\n",
        "            FacenetLayer(192, 256, kernel_size=3, stride=2)\n",
        "        )\n",
        "        self.branch2 = nn.MaxPool2d(3, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y0 = self.branch0(x)\n",
        "        y1 = self.branch1(x)\n",
        "        y2 = self.branch2(x)\n",
        "        out = torch.cat((y0, y1, y2), 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class InceptionResNetV2(nn.Module):\n",
        "    def __init__(self, num_classes=None, pretrained=None):\n",
        "        super(InceptionResNetV2, self).__init__()\n",
        "        self.convo2d_1a = FacenetLayer(3, 32, kernel_size=3, stride=2)\n",
        "        self.conv2d_2a = FacenetLayer(32, 32, kernel_size=3, stride=1)\n",
        "        self.conv2d_2b = FacenetLayer(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\n",
        "        self.conv2d_3b = FacenetLayer(64, 80, kernel_size=1, stride=1)\n",
        "        self.conv2d_4a = FacenetLayer(80, 192, kernel_size=3, stride=1)\n",
        "        self.conv2d_4b = FacenetLayer(192, 256, kernel_size=3, stride=2)\n",
        "\n",
        "        self.repeat_1 = nn.Sequential(\n",
        "            Block35(scale=0.17),\n",
        "            Block35(scale=0.17),\n",
        "            Block35(scale=0.17),\n",
        "            Block35(scale=0.17),\n",
        "            Block35(scale=0.17)\n",
        "        )\n",
        "\n",
        "        self.mixed_6a = Mixed_6a()\n",
        "        self.repeat_2 = nn.Sequential(\n",
        "            Block17(scale=0.10),\n",
        "            Block17(scale=0.10),\n",
        "            Block17(scale=0.10),\n",
        "            Block17(scale=0.10),\n",
        "            Block17(scale=0.10),\n",
        "            Block17(scale=0.10),\n",
        "            Block17(scale=0.10),\n",
        "            Block17(scale=0.10),\n",
        "            Block17(scale=0.10),\n",
        "            Block17(scale=0.10)\n",
        "        )\n",
        "\n",
        "        self.mixed_7a = Mixed_6a()\n",
        "        self.repeat_3 = nn.Sequential(\n",
        "            Block8(scale=0.20),\n",
        "            Block8(scale=0.20),\n",
        "            Block8(scale=0.20),\n",
        "            Block8(scale=0.20),\n",
        "            Block8(scale=0.20)\n",
        "        )\n",
        "\n",
        "        self.block8 = Block8(noReLU=True)\n",
        "        self.avgpool_1a = nn.AdaptiveAvgPool2d(1)\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "        self.last_linear = nn.Linear(1792, 128, bias=False)\n",
        "        self.last_bn = nn.BatchNorm1d(128, eps=0.001, momentum=0.1, affine=True)\n",
        "\n",
        "        self.device = torch.device('cpu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convo2d_1a(x)\n",
        "        x = self.conv2d_2a(x)\n",
        "        x = self.conv2d_2b(x)\n",
        "        x = self.maxpool_3a(x)\n",
        "        x = self.conv2d_3b(x)\n",
        "        x = self.conv2d_4a(x)\n",
        "        x = self.conv2d_4b(x)\n",
        "        x = self.repeat_1(x)\n",
        "        x = self.mixed_6a(x)\n",
        "        x = self.repeat_2(x)\n",
        "        x = self.mixed_7a(x)\n",
        "        x = self.repeat_3(x)\n",
        "        x = self.block8(x)\n",
        "        x = self.avgpool_1a(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.last_linear(x.view(x.shape[0], -1))\n",
        "        x = self.last_bn(x)\n",
        "\n",
        "        return F.normalize(x, p=2, dim=1)\n"
      ],
      "metadata": {
        "id": "atJyEZRyAyEH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOADING DATASET AND EXTRACTING EMBEDDINGS**"
      ],
      "metadata": {
        "id": "vM6UtijQrfLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transforms.ToTensor()# convert image to tensor\n",
        "#Normalize the image\n",
        "transform = transforms.Compose([\n",
        "transforms.Normalize(mean = [0.5,0.5,0.5], std = [0.5,0.5,0.5])])\n",
        "\n",
        "\n",
        "#Loading the dataset\n",
        "dataset = datasets.ImageFolder(root =\"/content/drive/MyDrive/archive (2)/TransformedData\",transform = transform)\n",
        "''' from the beginning of this block to this part , the changes were made by copilot ai'''\n",
        "\n",
        "#Extracting Embeddings\n",
        "def extract_embeddings(model, dataloader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs = inputs.to(model.device)\n",
        "            outputs = model(inputs)\n",
        "            embeddings.append(outputs.cpu().numpy())\n",
        "            labels.append(targets.cpu().numpy())\n",
        "\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    labels = np.concatenate(labels)\n",
        "\n",
        "    return embeddings, labels\n",
        "\n",
        "# Initializing the model\n",
        "model = InceptionResNetV2(num_classes=1000)\n",
        "model = model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "\n",
        "# Extract embeddings\n",
        "embeddings, labels = extract_embeddings(model, Dataloader)\n",
        "\n",
        "# Save embeddings to file\n",
        "import numpy as np\n",
        "np.save('embeddings.npy', embeddings)\n",
        "np.save('labels.npy', labels)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zAeMne2dr2iS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}